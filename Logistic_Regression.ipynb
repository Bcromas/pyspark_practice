{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A client wants to reduce customer churn by determining how reliably it can be predicted and intervening in situations wheew a client is likely to leave them. We'll use the data in customer_churn.csv to fit and evaluate a model and then generate predictions using the data in new_customers.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/bryan/Documents/Code/spark-2.4.5-bin-hadoop2.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('lr_churn').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"data/customer_churn.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data.count() == data.na.drop().count(), \"Check for missing data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### According to the client, whether a client has an account manager or not is determined randomly. Can we see, based on the data, whether this random phenomenon has a positive or negative impact on churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(data.select('Company').distinct().count()/data.count(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### The Company column is effectively a unique value and may not help us in predicting churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|Location                                               |\n",
      "+-------------------------------------------------------+\n",
      "|10265 Elizabeth Mission Barkerburgh, AK 89518          |\n",
      "|6157 Frank Gardens Suite 019 Carloshaven, RI 17756     |\n",
      "|1331 Keith Court Alyssahaven, DE 90114                 |\n",
      "|13120 Daniel Mount Angelabury, WY 30645-4695           |\n",
      "|765 Tricia Row Karenshire, MH 71730                    |\n",
      "|6187 Olson Mountains East Vincentborough, PR 74359     |\n",
      "|4846 Savannah Road West Justin, IA 87713-3460          |\n",
      "|25271 Roy Expressway Suite 147 Brownport, FM 59852-6150|\n",
      "|3725 Caroline Stravenue South Christineview, MA 82059  |\n",
      "|363 Sandra Lodge Suite 144 South Ann, WI 51655-7561    |\n",
      "+-------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('Location').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### The locations appear to be U.S.-based. Would it be helpful to generate new State and/or Zip Code features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary| Onboard_date_year|\n",
      "+-------+------------------+\n",
      "|  count|               900|\n",
      "|   mean|2010.8011111111111|\n",
      "| stddev|3.2072288498508783|\n",
      "|    min|              2006|\n",
      "|    max|              2016|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('Onboard_date_year', F.year('Onboard_date'))\n",
    "data.select('Onboard_date_year').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Onboard_date covers a period of approximately 10 years from 2006 to 2016. Below we will verify whether the columns Years and Onboard_date are reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       Years_check|\n",
      "+-------+------------------+\n",
      "|  count|               900|\n",
      "|   mean|2016.0742666666667|\n",
      "| stddev|3.3954957103523697|\n",
      "|    min|           2008.41|\n",
      "|    max|            2023.6|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('Years_check', F.col('Onboard_date_year')+F.col('Years'))\n",
    "data.select('Years_check').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-----------+\n",
      "|Years|Onboard_date_year|Years_check|\n",
      "+-----+-----------------+-----------+\n",
      "| 6.67|             2016|    2022.67|\n",
      "| 5.56|             2016|    2021.56|\n",
      "| 5.23|             2016|    2021.23|\n",
      "| 5.22|             2016|    2021.22|\n",
      "| 6.64|             2015|    2021.64|\n",
      "+-----+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(F.col('Years_check')>2021).select(['Years', 'Onboard_date_year', 'Years_check']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filter(F.col('Years_check')>2021).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### At least 60 records have a combination of 'Years' and 'Onboard_date' that indicates the client has been with the company beyond the year 2020. This does not make sense. However, we'll simply use the 'Years' column in the interest of this toy exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: State & Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_pattern = r\"([A-Z]{2}) (\\d{5})\"\n",
    "data = data.withColumn('Location_state', F.regexp_extract(F.col('Location'),this_pattern,1))\\\n",
    ".withColumn('Location_zip', F.regexp_extract(F.col('Location'),this_pattern,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------+------------+\n",
      "|Location                                          |Location_state|Location_zip|\n",
      "+--------------------------------------------------+--------------+------------+\n",
      "|10265 Elizabeth Mission Barkerburgh, AK 89518     |AK            |89518       |\n",
      "|6157 Frank Gardens Suite 019 Carloshaven, RI 17756|RI            |17756       |\n",
      "|1331 Keith Court Alyssahaven, DE 90114            |DE            |90114       |\n",
      "|13120 Daniel Mount Angelabury, WY 30645-4695      |WY            |30645       |\n",
      "|765 Tricia Row Karenshire, MH 71730               |MH            |71730       |\n",
      "+--------------------------------------------------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(['Location', 'Location_state', 'Location_zip']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Location_state|count|\n",
      "+--------------+-----+\n",
      "|            AA|   44|\n",
      "|            AP|   36|\n",
      "|            AE|   27|\n",
      "|            OK|   22|\n",
      "|            TN|   18|\n",
      "|            WV|   18|\n",
      "|            SC|   18|\n",
      "|            ID|   17|\n",
      "|            LA|   17|\n",
      "|            ME|   17|\n",
      "+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('Location_state').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Location_zip|count|\n",
      "+------------+-----+\n",
      "|       19037|    2|\n",
      "|       12062|    2|\n",
      "|       92778|    2|\n",
      "|       26077|    2|\n",
      "|       68644|    2|\n",
      "|       62096|    2|\n",
      "|       57273|    1|\n",
      "|       81808|    1|\n",
      "|       01493|    1|\n",
      "|       58602|    1|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('Location_zip').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### It appears the state attribute from 'Location' may be useful for our model. The zip code attribute is likely too unique to help fit a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn',\n",
       " 'Onboard_date_year',\n",
       " 'Years_check',\n",
       " 'Location_state',\n",
       " 'Location_zip']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------+-----+---------+--------------+\n",
      "| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Location_state|\n",
      "+----+--------------+---------------+-----+---------+--------------+\n",
      "|42.0|       11066.8|              0| 7.22|      8.0|            AK|\n",
      "|41.0|      11916.22|              0|  6.5|     11.0|            RI|\n",
      "|38.0|      12884.75|              0| 6.67|     12.0|            DE|\n",
      "+----+--------------+---------------+-----+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(these_cols).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_indexer = StringIndexer(inputCol='Location_state', outputCol='StateIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_encoder = OneHotEncoder(inputCol='StateIndex',outputCol='StateVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\n",
    "    'Age',\n",
    "    'Total_Purchase',\n",
    "    'Account_Manager',\n",
    "    'Years',\n",
    "    'Num_Sites',\n",
    "    'StateVec'\n",
    "], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_base = LogisticRegression(featuresCol='features',labelCol='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[state_indexer,state_encoder,\n",
    "                           assembler,log_reg_base])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_base_fit = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_base_results = log_reg_base_fit.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Churn|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_base_results.select('Churn','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7061752988047808"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC = base_eval.evaluate(log_reg_base_results)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
